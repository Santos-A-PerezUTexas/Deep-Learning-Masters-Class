# UT Fall 2021 Deep Learning Final Project

#### Team Members: Santos A. Perez | Yilong Chen | Weizhe Tang           

## Introduction 
Bulding a good AI model sucessfully playing in a game competition has always been a formidable task  for game companies and machine learning engineers. The famous competition between  Alpha Go and Ke Jie  indeed proves that a machine can do better job than humans in some game competitions. Along those lines, in this project, we are tasked with building an agent which could compete with various AI models in a 2x2 competition in the SuperTuxIcehockey game. Although we've never played this game, when we watched the demo it seemed similar to a soccer competition.  With this knowledge,  we began  by first dividing our group into two sub-groups, to explore more solutions. Mr. Santos Perez  was group1 (along with someone else who dropped course), and Mr. Weizhe Tang and Mr. Yilong Chen  in group2.  We worked together by holding weekly zoom meetings to discuss progress and exchange ideas. In the end, with this approach, we decided to solve the problem with an image based agent.


## Originality
### Data collection
#### 1 We decided to use the VideoRecorder class, since this would allow us to store both render_data instance labels, as well as soccer state.

#### 2  A class method called _to_image300_400(params) was implemented, which is the same as _to_image from HW5,  but it converts -1..1 image coordinates to 300/400 image coordinates as depicted in the project slides (x 400 range, y 300 range), with the 0,0 coordinate now at the top left.

#### 3 The collect() method from HW5 was then invoked, and was called with the soccer state  and the render_data instance as params, as well as the image and proj/view for team1 or team2 images.
#### 4 Data set consisted of  10000 image/labels pairs.

#### 5  Labels were then generated with either the soccer/puck location, or the render_data instance (bitshifted by 24), which in the end we decide not to use. The final solution uses a modified _to_image()_ to convert world coords to image coords (labels) in the 400/300 range.

### Planner
#### 1  The planner had to meet the timeout limitations, and a model which closely resembles HW4 was used for that purpose, and was tested and complied with time limits.  
#### 2  The method spatial_argmax() was borrowed from HW5, but the points were normalized/converted to  300/400 range after after the call to spatial_argmax, and that normalized output was then returned by the model.  Training the model was similar to HW5 training. 

### Controller
#### Since we choose to use image agent, we have very limited information about the puck. We could only get x, y coordinates predicted by the model and we found that y did not influence the performance much in practice, so we just use the x coordinate in the controller.
#### We set the center of the picture to 0, 0 and make the x coordinate scaled to -1~1 and use this value as the sin(angle) which is then used to calculate the angle which should be a value between -90 to 90. We found that the effect of steer value, acceleration value, drift value are closely related to this angle, so the main task is remained to tune the threshold angle value for these actions if we want to make the car head to the ball.
#### Although we have had a basic logic, we still need some specific strategies to balance the negative influence made by this logic. There are two main problems. The first one is that if the car hit a wall or just go over the goal line by chance, it will need some methods to make a recovery. We use a backcount parameter to solve this problem. If the carâ€™s location does not change since last frame, we would increase the backcount value. If the backcount value is larger than a certain number, we will make recovery while decreasing the backcount number. After the turn is over, we set the value back to 0. Another problem is that if the car is always heading to the ball, they will have a large opportunity to kick the ball into the wrong goal line. To decrease the possibility for this condition, we use a turncount parameter to count the continuous identical direction of turns the car has made. Just like the backcount parameter, when turncount value exceeds a certain number, we will make the car to do a recovery at the opposite side of the goal line. And then the direction of the car will be changed to the direction facing the goal line. To achieve this, we need to set the steer less accurate when the car is heading to the ball in the wrong direction. This will make the car fail to hit the center of the ball and run around the ball just like a satellite. By combing the x, y direction in front of the car as well as the steer direction, the car will be able to make a recovery and hit the ball backwards since in the right direction towards to the goal line we does not tune the accuracy. 

## Exploration period(before the mid of second week(10 days))
### Group1
The first group decided to do an image based agent. His main idea is similar to the HW5,  to use the planner to predict/detect the image location of the puck, then use controller to steer toward the puck.
First week's main problem for Santos is collecting the data. The  difficulties he encountered is (1) whether to use image or world coords for puck (2) how to convert the world coordinate to image coordinate, (3) Whether to predict render_data bitshift or image coords, (4) whether to use -1..1 coords or 300/400, (5) whether to predict puck_in_image flag/classification.  
Solutions: (*1) Early on, it was decided not to predict the world coordinates, since we wanted a solution similar to HW5. (*2) image_coords were generated using a modified version of HW5 _to_image()_, which converted -1..1 range to 300/400 range as per the Project slides.   (*3)Bitshifted  Render_data instance labels were succesfully generated (4K image/labels), but during training after 80 epochs the loss increased, and model predicted a 3 object as a 6 object (for instance).  So it was decided to use image coords as labels instead of render_data instance.  (*4) 300/400 range for image coords was used because the image is 400,300 (x,y), and the slides made use of this range, which would also have allowed us to use render_data instances to predict location of '8' object (which we decided not to use in the end).  (*5) render_data instances were the best choice for puck_in_image classification, since a modified version of _to_image_ did not properly classify puck based on range > 400 (or abs(x)>1 in in the -1..1 range).  However, as the controller worked without predicting puck_in_image, we decided not to classify puck.   

### Group2
Initially, Yilong and Weizhe also considered the Image agent, but they did not see the Image agent as an optimal solution, so they considered a state agent using q-learning. After research and testing, however, they  found the q-learning might not be a good solution for this particular game. The main problem they encountered was how to design a reward.   Mr. Yilong came up with a clever idea,  that he could use the summation of (the distance between the ball and the goal) and (the distance between the kart and the ball) to measure the reward. I.E. reward = previous_distance - current_distance, if the distance becomes smaller, there is a reward. But a problem they encountered was that there are plenty of different states, the puck's location, kart's velocity, kart's location, kart's front, to name a few. This was a problem since  they are using a q-table, and the size of the table could become extremely large. Mr.  Weizhe suggested a network to replace the q-table as a solution. However,  a problem with the network approac was  designing a loss function.  More specifically, where can they get labels if they wanted to return an action? Mr. Yilong came up with a different solution, that they should firstly try to approximate the state value into nearest place. I.E. 3.66 to 3.6, then their table will become small. This solution seemed tractable, so they chose Yilong's Idea. They built and trained the model and, although it does not perform well (scored goals), it was learning and the opponent's score wass not as high as previous scores. Afterwards, they did some analysis on the recording, they found that the kart seems to follow certain pattern after few hundred episodes. Mr. Weizhe then proposed that the model should be given right to try more new things during first few episode, so he added a decay_factor for the action randomness factor and increase the action randomness factor in initial stages of training. Although they see karts has more option in action, however it does not imporve the final results. Then they tried to modify the reward and tried to add the final results to the reward system. Mr. Yilong thought if the game was won,it should reward the fianl action to a very large number like 1000. Mr. Weizhe thought that if the game was lost, some of the important actions should be punished. He tried to write a memory list that containeds previous n-steps actions and punish them all. However, this  solution performed poorly. After further analysis, they decided q learning was not good enough for the task, since its table is discrete and our task is continuous. Plus, we chunk our state to the nearest number which makes this situation even worse, I.E. we will do same action in the similar location, which will not fit to our complex environment. Then we try to investigate the DQN approach.

## Summary 
We tried Q learning, and our model followed the puck and did not score, because of the problem of defining a reward.  We also had some problems with the image agent, since there were many approaches in what to predict (world coords, image coords, render_data instance, image_coords normalized to 400/300, puck classification as in image, not in image).  Primarly because of time limitations, and slow training time, we decided to keep it simple with an image-based approach predicting image coordinates, although we did try to predict render_data instances, as well as to classify the puck.  But in the end, using a simple, robust approach similar to HW5, gave us results, and we simply predicted 400/300 image coords (since this would have allowed us also to use render_data instance), and employed a simple controller approach which steered in the direction of the image coords.




